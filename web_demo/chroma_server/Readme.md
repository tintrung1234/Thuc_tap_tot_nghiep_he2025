# ğŸ“Œ Blog Search with ChromaDB + Vietnamese NLP

Dá»± Ã¡n nÃ y xÃ¢y dá»±ng há»‡ thá»‘ng **tÃ¬m kiáº¿m thÃ´ng minh** cho blog tiáº¿ng Viá»‡t,
sá»­ dá»¥ng: - **Chunking + Vietnamese NLP** (underthesea) -
**Embedding** báº±ng Sentence-BERT - **Vector Database** vá»›i
[ChromaDB](https://docs.trychroma.com/)

---

## ğŸš€ CÃ´ng nghá»‡ sá»­ dá»¥ng

- [Python 3.11](https://www.python.org/)
- [ChromaDB](https://github.com/chroma-core/chroma) -- lÆ°u trá»¯ vector
  embedding
- [Sentence-Transformers](https://www.sbert.net/) -- táº¡o embeddings
- [underthesea](https://github.com/undertheseanlp/underthesea) -- phÃ¢n
  cÃ¢u

---

## âš™ï¸ CÃ i Ä‘áº·t

1.  **Clone repo**

    ```bash
    git clone https://github.com/yourname/blog-chroma-demo.git
    cd blog-chroma-demo
    ```

2.  **Táº¡o mÃ´i trÆ°á»ng áº£o & cÃ i dependencies**

    ```bash
    python -m venv .venv
    source .venv/bin/activate   # Linux / MacOS
    .venv\Scripts\activate      # Windows

    pip install -r requirements.txt
    ```

3.  **Cáº¥u hÃ¬nh**

        - File `.env` chá»©a config (vÃ­ dá»¥ `DB_PATH`, `EMBEDDING_MODEL`).

```bash
    CHROMA_COLLECTION=
    EMBED_MODEL=
    MAX_TOKENS=
    OVERLAP=
    BATCH_SIZE=
    CHROMA_PATH=
    MONGO_URI=
    MONGO_DB=
    MONGO_COLLECTION=
```

---

## ğŸ—ï¸ Pipeline ETL

Pipeline náº±m trong thÆ° má»¥c `etl/`: - `text_processing.py` â†’ xá»­ lÃ½ vÄƒn
báº£n thÃ´ - `chunking.py` â†’ chia nhá» vÄƒn báº£n thÃ nh chunks - `embedding.py`
â†’ táº¡o vector embedding tá»« Sentence-BERT - `database.py` â†’ insert/update
dá»¯ liá»‡u vÃ o ChromaDB - `main.py` â†’ script chÃ­nh Ä‘á»ƒ cháº¡y ETL

Cháº¡y ETL Ä‘á»ƒ load dá»¯ liá»‡u vÃ o Chroma:

```bash
python -m etl.main
```

---

## ğŸ” Query dá»¯ liá»‡u

VÃ­ dá»¥ tÃ¬m kiáº¿m:

```python
import chromadb
from sentence_transformers import SentenceTransformer

# 1. Káº¿t ná»‘i tá»›i collection
client = chromadb.PersistentClient(path="./chroma-data")
collection = client.get_collection("blog_vi")

print(f"Collection size: {collection.count()}")

# 2. Load embedding model
model = SentenceTransformer(
    "sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2")

# 3. Náº¿u collection trá»‘ng thÃ¬ bÃ¡o lá»—i
if collection.count() == 0:
    print("âš ï¸ Collection 'blog_vi' chÆ°a cÃ³ dá»¯ liá»‡u. Báº¡n cáº§n insert chunks trÆ°á»›c khi query.")
else:
    # 4. Thá»±c hiá»‡n query
    query = "áº¨m thá»±c HÃ  Ná»™i"
    query_emb = model.encode(
        [query], convert_to_numpy=True, normalize_embeddings=True)

    results = collection.query(
        query_embeddings=query_emb.tolist(), n_results=3)

    # 5. Kiá»ƒm tra dá»¯ liá»‡u tráº£ vá»
    if results["documents"] is None:
        print("âŒ KhÃ´ng tÃ¬m tháº¥y káº¿t quáº£ nÃ o khá»›p vá»›i query.")
    elif results['metadatas'] is None:
        print("âŒ KhÃ´ng tÃ¬m tháº¥y káº¿t quáº£ nÃ o khá»›p vá»›i query.")
    else:
        for doc, meta in zip(results['documents'][0], results['metadatas'][0]):
            print(f"Title: {meta['title']}, Chunk: {doc}")
```

---

## ğŸ“‚ Cáº¥u trÃºc thÆ° má»¥c

    â”œâ”€â”€ .venv/
    â”œâ”€â”€ chroma-data/           # vector database (persistent)
    â”œâ”€â”€ etl/                   # ETL pipeline
    â”‚   â”œâ”€â”€ __init__.py
    â”‚   â”œâ”€â”€ chunking.py
    â”‚   â”œâ”€â”€ config.py
    â”‚   â”œâ”€â”€ database.py
    â”‚   â”œâ”€â”€ embedding.py
    â”‚   â”œâ”€â”€ main.py
    â”‚   â””â”€â”€ text_processing.py
    â”œâ”€â”€ requirements.txt
    â”œâ”€â”€ .env
    â””â”€â”€ README.md

---

## ğŸ› ï¸ Troubleshooting

- **`ModuleNotFoundError: No module named 'etl'`** â†’ cháº¡y script báº±ng
  `python -m etl.main` thay vÃ¬ `python etl/main.py`.

- **Chroma khÃ´ng cÃ³ dá»¯ liá»‡u**\
  â†’ Ä‘áº£m báº£o Ä‘Ã£ cháº¡y ETL Ä‘á»ƒ náº¡p dá»¯ liá»‡u (`python -m etl.main`).

---

## ğŸ“œ License

MIT License Â© 2025
